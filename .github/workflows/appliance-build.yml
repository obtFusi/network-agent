name: Appliance Build

# FAST APPLIANCE BUILD (uses pre-built base image)
# Only adds Network Agent layer to existing base:
# - Docker Compose files + configs
# - First-boot setup + Firewall
# - Pull Network Agent Docker images
#
# Build time: ~5 min (vs ~40 min from scratch)
#
# Prerequisites:
# - Base image in MinIO appliance-base/ bucket
# - Build base image first with appliance-base-build.yml if needed

on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Version (e.g., 0.10.1)'
        required: true
        type: string
      skip_test:
        description: 'Skip E2E test (for debugging)'
        type: boolean
        default: false
      base_image:
        description: 'Base image name (default: latest)'
        type: string
        default: ''
  release:
    types: [published]

concurrency:
  group: appliance-build
  cancel-in-progress: false

env:
  PACKER_VERSION: "1.11.0"
  TEST_VMID: 999
  TELEMETRY_DIR: /tmp/telemetry
  TELEMETRY_BUCKET: appliance-telemetry

permissions:
  contents: write

jobs:
  validate:
    name: Validate Templates
    runs-on: ubuntu-latest
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@v2
        with:
          egress-policy: audit

      - uses: actions/checkout@v4

      - name: Setup Packer
        uses: hashicorp/setup-packer@v3
        with:
          version: ${{ env.PACKER_VERSION }}

      - name: Initialize Packer plugins
        working-directory: infrastructure/packer
        run: packer init appliance.pkr.hcl

      - name: Validate Packer templates
        working-directory: infrastructure/packer
        run: |
          # Validate appliance template (uses base image)
          packer validate \
            -var "version=0.0.0" \
            -var "base_image_url=file:///tmp/test.qcow2" \
            appliance.pkr.hcl

      - name: Validate docker-compose files
        run: |
          docker compose -f infrastructure/docker/docker-compose.yml config --quiet
          docker compose -f infrastructure/docker/docker-compose.yml \
                        -f infrastructure/docker/docker-compose.scan-mode.yml config --quiet
          docker compose -f infrastructure/docker/docker-compose.yml \
                        -f infrastructure/docker/docker-compose.online.yml config --quiet

  # ═══════════════════════════════════════════════════════════════════════════
  # BUILD JOB: Fast appliance build using pre-built base image
  # ═══════════════════════════════════════════════════════════════════════════
  build:
    name: Build Appliance
    runs-on: self-hosted
    needs: validate
    timeout-minutes: 30
    outputs:
      version: ${{ steps.version.outputs.VERSION }}
      artifact-name: ${{ steps.version.outputs.ARTIFACT_NAME }}
    env:
      VERSION: ${{ inputs.version || github.ref_name }}
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@v2
        with:
          egress-policy: audit
          allowed-endpoints: >
            github.com:443
            api.github.com:443
            ghcr.io:443
            registry-1.docker.io:443
            auth.docker.io:443
            production.cloudflare.docker.com:443
            dl.min.io:443

      - uses: actions/checkout@v4

      # ═══════════════════════════════════════════════════════════
      # TELEMETRY INITIALIZATION
      # ═══════════════════════════════════════════════════════════

      - name: Initialize telemetry
        run: |
          chmod +x infrastructure/scripts/telemetry.sh
          source infrastructure/scripts/telemetry.sh
          telemetry_init "build" "${VERSION#v}"
          cp infrastructure/scripts/telemetry.sh /tmp/telemetry.sh

      # ═══════════════════════════════════════════════════════════
      # VERSION & DEPENDENCIES
      # ═══════════════════════════════════════════════════════════

      - name: Set version output
        id: version
        run: |
          VERSION_CLEAN="${VERSION#v}"
          echo "VERSION=$VERSION_CLEAN" >> $GITHUB_OUTPUT
          echo "ARTIFACT_NAME=appliance-${VERSION_CLEAN}" >> $GITHUB_OUTPUT
          echo "Building version: $VERSION_CLEAN"

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y unzip zstd qemu-utils bc jq

      - name: Setup Packer
        uses: hashicorp/setup-packer@v3
        with:
          version: ${{ env.PACKER_VERSION }}

      - name: Verify KVM support
        run: |
          if [[ ! -e /dev/kvm ]]; then
            echo "ERROR: KVM not available"
            exit 1
          fi

      # ═══════════════════════════════════════════════════════════
      # DOWNLOAD BASE IMAGE FROM MINIO
      # ═══════════════════════════════════════════════════════════

      - name: Download base image
        id: base-image
        env:
          MINIO_ENDPOINT: ${{ secrets.MINIO_ENDPOINT }}
          MINIO_ACCESS_KEY: ${{ secrets.MINIO_ACCESS_KEY }}
          MINIO_SECRET_KEY: ${{ secrets.MINIO_SECRET_KEY }}
          BASE_IMAGE_INPUT: ${{ inputs.base_image }}
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "base_image_download"

          wget -q https://dl.min.io/client/mc/release/linux-amd64/mc -O /tmp/mc
          chmod +x /tmp/mc
          /tmp/mc alias set minio $MINIO_ENDPOINT $MINIO_ACCESS_KEY $MINIO_SECRET_KEY

          # Find base image (use input or latest)
          if [[ -n "$BASE_IMAGE_INPUT" ]]; then
            BASE_IMAGE="$BASE_IMAGE_INPUT"
          else
            BASE_IMAGE=$(/tmp/mc ls minio/appliance-base/ 2>/dev/null | grep -o 'debian-docker-ollama-[0-9]*\.qcow2' | sort -r | head -1 || echo "")
          fi

          if [[ -z "$BASE_IMAGE" ]]; then
            echo "::error::No base image found in MinIO!"
            echo "Run 'appliance-base-build.yml' workflow first to create a base image."
            exit 1
          fi

          echo "Using base image: $BASE_IMAGE"
          mkdir -p infrastructure/packer/input
          /tmp/mc cp minio/appliance-base/$BASE_IMAGE infrastructure/packer/input/

          # Verify download
          ls -lh infrastructure/packer/input/$BASE_IMAGE
          echo "BASE_IMAGE_PATH=$(pwd)/infrastructure/packer/input/$BASE_IMAGE" >> $GITHUB_OUTPUT

          telemetry_step_end "base_image_download"

      - name: Initialize Packer
        working-directory: infrastructure/packer
        run: packer init appliance.pkr.hcl

      # ═══════════════════════════════════════════════════════════
      # BUILD PHASE (add Network Agent layer to base image)
      # ═══════════════════════════════════════════════════════════

      - name: Build appliance
        working-directory: infrastructure/packer
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "packer_build"

          VERSION_CLEAN="${VERSION#v}"
          BASE_IMAGE_PATH="${{ steps.base-image.outputs.BASE_IMAGE_PATH }}"

          echo "Building Network Agent v$VERSION_CLEAN on base image..."
          packer build \
            -var "version=${VERSION_CLEAN}" \
            -var "base_image_url=file://${BASE_IMAGE_PATH}" \
            -on-error=abort \
            appliance.pkr.hcl 2>&1 | tee /tmp/packer-build.log

          telemetry_step_end "packer_build"

      - name: Verify build output
        run: |
          ls -lh infrastructure/packer/output/
          QCOW2_FILE=$(ls infrastructure/packer/output/*.qcow2)
          echo "Built image: $QCOW2_FILE"
          df -h .

      - name: Split into parts
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "split_parts"

          cd infrastructure/packer/output
          VERSION_CLEAN="${VERSION#v}"

          QCOW2=$(ls *.qcow2)
          mv "$QCOW2" "network-agent-${VERSION_CLEAN}.qcow2"

          echo "Splitting into 1.9GB parts..."
          split -b 1900M -a 2 "network-agent-${VERSION_CLEAN}.qcow2" "network-agent-${VERSION_CLEAN}.qcow2.part-"

          # Rename numeric suffixes to alpha
          for f in *.part-[0-9][0-9]; do
            if [[ -f "$f" ]]; then
              num="${f##*.part-}"
              first=$(printf "\\x$(printf '%02x' $((97 + 10#$num / 26)))")
              second=$(printf "\\x$(printf '%02x' $((97 + 10#$num % 26)))")
              mv "$f" "${f%.part-*}.part-${first}${second}"
            fi
          done

          rm -f "network-agent-${VERSION_CLEAN}.qcow2"
          echo "Parts created:"
          ls -lh *.part-*

          telemetry_step_end "split_parts"

      - name: Generate checksums
        run: |
          cd infrastructure/packer/output
          sha256sum *.part-* > SHA256SUMS
          cat SHA256SUMS

      - name: Copy install script
        run: |
          VERSION_CLEAN="${VERSION#v}"
          cp infrastructure/scripts/install-network-agent.sh infrastructure/packer/output/
          sed -i "s/VERSION:-0.4.0/VERSION:-${VERSION_CLEAN}/" infrastructure/packer/output/install-network-agent.sh

      # ═══════════════════════════════════════════════════════════
      # UPLOAD TO MINIO
      # ═══════════════════════════════════════════════════════════

      - name: Upload to MinIO
        env:
          MC_HOST_local: "${{ secrets.MINIO_ENDPOINT }}|${{ secrets.MINIO_ACCESS_KEY }}|${{ secrets.MINIO_SECRET_KEY }}"
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "minio_upload"

          /tmp/mc alias set minio ${{ secrets.MINIO_ENDPOINT }} ${{ secrets.MINIO_ACCESS_KEY }} ${{ secrets.MINIO_SECRET_KEY }}

          VERSION_CLEAN="${VERSION#v}"
          /tmp/mc cp --recursive infrastructure/packer/output/ minio/appliance-builds/${VERSION_CLEAN}/

          echo "Uploaded to MinIO:"
          /tmp/mc ls minio/appliance-builds/${VERSION_CLEAN}/

          telemetry_step_end "minio_upload"

      - name: Finalize and upload telemetry
        if: always()
        env:
          MINIO_ENDPOINT: ${{ secrets.MINIO_ENDPOINT }}
          MINIO_ACCESS_KEY: ${{ secrets.MINIO_ACCESS_KEY }}
          MINIO_SECRET_KEY: ${{ secrets.MINIO_SECRET_KEY }}
        run: |
          source /tmp/telemetry.sh
          telemetry_finalize

          VERSION_CLEAN="${VERSION#v}"
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)

          /tmp/mc alias set minio $MINIO_ENDPOINT $MINIO_ACCESS_KEY $MINIO_SECRET_KEY 2>/dev/null || true
          /tmp/mc mb --ignore-existing minio/$TELEMETRY_BUCKET 2>/dev/null || true

          if [[ -d "$TELEMETRY_DIR" ]]; then
            /tmp/mc cp --recursive $TELEMETRY_DIR/ minio/$TELEMETRY_BUCKET/${VERSION_CLEAN}/${TIMESTAMP}/ 2>/dev/null || true
          fi

          TELEMETRY_JSON=$(ls $TELEMETRY_DIR/*.json 2>/dev/null | head -1)
          if [[ -f "$TELEMETRY_JSON" ]]; then
            /tmp/mc cp "$TELEMETRY_JSON" minio/$TELEMETRY_BUCKET/latest/build_${VERSION_CLEAN}.json 2>/dev/null || true
          fi

      - name: Cleanup build directory
        if: always()
        run: |
          rm -rf infrastructure/packer/output/ || true
          rm -rf infrastructure/packer/input/ || true
          sudo fstrim -av || true

  # ═══════════════════════════════════════════════════════════════════════════
  # E2E TEST JOB (unchanged from original)
  # ═══════════════════════════════════════════════════════════════════════════
  e2e-test:
    name: E2E Test
    runs-on: self-hosted
    needs: build
    if: ${{ inputs.skip_test != true }}
    timeout-minutes: 30
    env:
      VERSION: ${{ needs.build.outputs.version }}
      ARTIFACT_NAME: ${{ needs.build.outputs.artifact-name }}
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@v2
        with:
          egress-policy: audit
          allowed-endpoints: >
            github.com:443
            api.github.com:443
            dl.min.io:443

      - uses: actions/checkout@v4

      - name: Initialize telemetry
        run: |
          chmod +x infrastructure/scripts/telemetry.sh
          source infrastructure/scripts/telemetry.sh
          telemetry_init "e2e_test" "${VERSION}"
          cp infrastructure/scripts/telemetry.sh /tmp/telemetry.sh

      - name: Validate Proxmox connectivity
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          if [[ -z "$PVE_HOST" ]]; then
            echo "::error::PVE_HOST secret is not configured!"
            exit 1
          fi

          if ! ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o BatchMode=yes root@$PVE_HOST "echo 'SSH OK'" 2>/dev/null; then
            echo "::error::Cannot SSH to Proxmox host $PVE_HOST"
            exit 1
          fi
          echo "✓ Proxmox connectivity validated"

      - name: Download from MinIO
        env:
          MINIO_ENDPOINT: ${{ secrets.MINIO_ENDPOINT }}
          MINIO_ACCESS_KEY: ${{ secrets.MINIO_ACCESS_KEY }}
          MINIO_SECRET_KEY: ${{ secrets.MINIO_SECRET_KEY }}
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "minio_download"

          wget -q https://dl.min.io/client/mc/release/linux-amd64/mc -O /tmp/mc
          chmod +x /tmp/mc
          /tmp/mc alias set minio $MINIO_ENDPOINT $MINIO_ACCESS_KEY $MINIO_SECRET_KEY

          mkdir -p infrastructure/packer/output
          /tmp/mc cp --recursive minio/appliance-builds/${VERSION}/ infrastructure/packer/output/

          cd infrastructure/packer/output
          sha256sum -c SHA256SUMS

          telemetry_step_end "minio_download"

      - name: Create test VM
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "vm_create"

          ssh -o StrictHostKeyChecking=no root@$PVE_HOST "qm stop $TEST_VMID 2>/dev/null || true"
          ssh root@$PVE_HOST "qm destroy $TEST_VMID --purge 2>/dev/null || true"

          BUILD_TMP="/root/build-tmp"
          ssh root@$PVE_HOST "mkdir -p $BUILD_TMP"

          cd infrastructure/packer/output
          cat *.part-* | ssh root@$PVE_HOST "cat > $BUILD_TMP/network-agent-test.qcow2"

          ssh root@$PVE_HOST "qm create $TEST_VMID \
            --name 'network-agent-test' \
            --memory 32768 \
            --cores 8 \
            --cpu host \
            --ostype l26 \
            --agent enabled=1 \
            --net0 virtio,bridge=vmbr0"

          ssh root@$PVE_HOST "qm importdisk $TEST_VMID $BUILD_TMP/network-agent-test.qcow2 local-lvm --format qcow2"

          ssh root@$PVE_HOST "DISK=\$(pvesm list local-lvm | grep 'vm-${TEST_VMID}-disk' | awk '{print \$1}') && \
            qm set $TEST_VMID --scsi0 \"\$DISK\" --boot order=scsi0 --scsihw virtio-scsi-single"

          ssh root@$PVE_HOST "qm start $TEST_VMID"
          ssh root@$PVE_HOST "rm -f $BUILD_TMP/network-agent-test.qcow2"

          telemetry_step_end "vm_create"

      - name: Wait for boot and get IP
        id: vm-ip
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "vm_boot_wait"

          sleep 60

          VM_IP=""
          for i in {1..30}; do
            VM_IP=$(ssh root@$PVE_HOST "qm guest cmd $TEST_VMID network-get-interfaces" 2>/dev/null | \
                    jq -r '.[1]["ip-addresses"][]? | select(.["ip-address-type"] == "ipv4") | .["ip-address"]' 2>/dev/null | head -1 || echo "")

            if [[ -n "$VM_IP" && "$VM_IP" != "null" && "$VM_IP" != "127.0.0.1" ]]; then
              echo "VM_IP=$VM_IP" >> $GITHUB_OUTPUT
              echo "VM IP: $VM_IP"
              break
            fi
            echo "Waiting for IP... ($i/30)"
            sleep 10
          done

          if [[ -z "$VM_IP" || "$VM_IP" == "null" ]]; then
            echo "ERROR: Failed to get VM IP"
            exit 1
          fi

          # Inject SSH key
          RUNNER_PUBKEY=$(ssh-keygen -y -f ~/.ssh/id_ed25519 2>/dev/null || cat ~/.ssh/id_ed25519.pub 2>/dev/null || echo "")
          if [[ -n "$RUNNER_PUBKEY" ]]; then
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -c 'mkdir -p /root/.ssh && chmod 700 /root/.ssh && echo \"$RUNNER_PUBKEY\" >> /root/.ssh/authorized_keys && chmod 600 /root/.ssh/authorized_keys'"
          fi

          # Regenerate SSH host keys
          ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -c 'ssh-keygen -A && systemctl restart sshd'"

          # Wait for SSH
          for i in {1..10}; do
            if ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 -o BatchMode=yes root@$VM_IP "echo SSH ready" 2>/dev/null; then
              break
            fi
            sleep 5
          done

          telemetry_step_end "vm_boot_wait"

      - name: Configure and start services
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "services_start"

          VM_IP="${{ steps.vm-ip.outputs.VM_IP }}"

          ssh -o StrictHostKeyChecking=no root@$VM_IP bash << 'EOF'
            printf '%s\n' \
              'POSTGRES_USER=agent' \
              'POSTGRES_PASSWORD=test-password-123' \
              'BASIC_AUTH_USER=admin' \
              'BASIC_AUTH_HASH=$2a$14$test.hash.for.testing.only' \
              > /opt/network-agent/.env
            chmod 600 /opt/network-agent/.env

            mkdir -p /var/lib/network-agent
            touch /var/lib/network-agent/.initialized

            cd /opt/network-agent
            docker compose up -d
          EOF

          telemetry_step_end "services_start"

      - name: Wait for services
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "services_wait"

          VM_IP="${{ steps.vm-ip.outputs.VM_IP }}"
          sleep 120
          ssh -o StrictHostKeyChecking=no root@$VM_IP "docker compose -f /opt/network-agent/docker-compose.yml ps"

          telemetry_step_end "services_wait"

      - name: Run health checks
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "health_checks"

          VM_IP="${{ steps.vm-ip.outputs.VM_IP }}"

          echo "=== Checking Ollama ==="
          if ssh -o StrictHostKeyChecking=no root@$VM_IP "docker exec ollama curl -sf http://localhost:11434/api/tags | grep -q qwen3"; then
            echo "✓ Ollama model loaded"
          else
            echo "✗ Ollama model not found"
            ssh root@$VM_IP "docker logs ollama --tail 50" || true
          fi

          echo "=== Checking PostgreSQL ==="
          if ssh root@$VM_IP "docker exec postgres pg_isready -U agent"; then
            echo "✓ PostgreSQL is ready"
          else
            echo "✗ PostgreSQL not ready"
          fi

          echo "=== Final status ==="
          ssh root@$VM_IP "docker compose -f /opt/network-agent/docker-compose.yml ps"

          telemetry_step_end "health_checks"

      - name: Finalize E2E telemetry
        if: always()
        env:
          MINIO_ENDPOINT: ${{ secrets.MINIO_ENDPOINT }}
          MINIO_ACCESS_KEY: ${{ secrets.MINIO_ACCESS_KEY }}
          MINIO_SECRET_KEY: ${{ secrets.MINIO_SECRET_KEY }}
        run: |
          source /tmp/telemetry.sh
          telemetry_finalize

          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          /tmp/mc alias set minio $MINIO_ENDPOINT $MINIO_ACCESS_KEY $MINIO_SECRET_KEY 2>/dev/null || true
          /tmp/mc mb --ignore-existing minio/$TELEMETRY_BUCKET 2>/dev/null || true

          if [[ -d "$TELEMETRY_DIR" ]]; then
            /tmp/mc cp --recursive $TELEMETRY_DIR/ minio/$TELEMETRY_BUCKET/${VERSION}/${TIMESTAMP}/ 2>/dev/null || true
          fi

          TELEMETRY_JSON=$(ls $TELEMETRY_DIR/*.json 2>/dev/null | head -1)
          if [[ -f "$TELEMETRY_JSON" ]]; then
            /tmp/mc cp "$TELEMETRY_JSON" minio/$TELEMETRY_BUCKET/latest/e2e_${VERSION}.json 2>/dev/null || true
          fi

      - name: Cleanup test VM
        if: success()
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          ssh root@$PVE_HOST "qm stop $TEST_VMID 2>/dev/null || true"
          sleep 5
          ssh root@$PVE_HOST "qm destroy $TEST_VMID --purge 2>/dev/null || true"
          rm -rf infrastructure/packer/output/ || true
          sudo fstrim -av || true

      - name: Cleanup on failure
        if: failure()
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          if [[ -n "$PVE_HOST" ]]; then
            ssh -o StrictHostKeyChecking=no root@$PVE_HOST "qm stop $TEST_VMID 2>/dev/null || true"
            ssh root@$PVE_HOST "qm destroy $TEST_VMID --purge 2>/dev/null || true"
          fi
          rm -rf infrastructure/packer/output/ || true
          sudo fstrim -av || true

      # ═══════════════════════════════════════════════════════════
      # UPLOAD TO GITHUB RELEASE
      # ═══════════════════════════════════════════════════════════

      - name: Upload to GitHub Release
        if: success() && github.event_name == 'release'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          cd infrastructure/packer/output
          gh release upload "${{ github.event.release.tag_name }}" \
            *.part-* \
            SHA256SUMS \
            install-network-agent.sh \
            --clobber

      - name: Cleanup MinIO artifacts
        if: always()
        env:
          MINIO_ENDPOINT: ${{ secrets.MINIO_ENDPOINT }}
          MINIO_ACCESS_KEY: ${{ secrets.MINIO_ACCESS_KEY }}
          MINIO_SECRET_KEY: ${{ secrets.MINIO_SECRET_KEY }}
        run: |
          /tmp/mc alias set minio $MINIO_ENDPOINT $MINIO_ACCESS_KEY $MINIO_SECRET_KEY 2>/dev/null || true
          /tmp/mc rm --recursive --force minio/appliance-builds/${VERSION}/ 2>/dev/null || true
