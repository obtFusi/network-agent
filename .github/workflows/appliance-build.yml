name: Appliance Build

on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Version (e.g., 0.4.0)'
        required: true
        type: string
      base_version:
        description: 'Base image version (e.g., 2026-01, or "latest")'
        type: string
        default: 'latest'
      skip_test:
        description: 'Skip E2E test (for debugging)'
        type: boolean
        default: false
      cleanup_only:
        description: 'Only cleanup old artifacts (no build)'
        type: boolean
        default: false
  release:
    types: [published]

concurrency:
  group: appliance-build
  cancel-in-progress: false

env:
  PACKER_VERSION: "1.11.0"
  TEST_VMID: 999

permissions:
  contents: write

jobs:
  validate:
    name: Validate Templates
    runs-on: ubuntu-latest
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@v2
        with:
          egress-policy: audit

      - uses: actions/checkout@v4

      - name: Setup Packer
        uses: hashicorp/setup-packer@v3
        with:
          version: ${{ env.PACKER_VERSION }}

      - name: Initialize Packer plugins
        working-directory: infrastructure/packer
        run: |
          # Init each template separately to avoid variable conflicts
          packer init layer.pkr.hcl
          packer init base.pkr.hcl

      - name: Validate Packer templates
        working-directory: infrastructure/packer
        run: |
          # Validate layer template (used for release builds)
          packer validate -var "version=0.0.0" -var "base_image=dummy.qcow2" layer.pkr.hcl
          # Validate base template (used for monthly builds)
          packer validate -var "base_version=2026-01" base.pkr.hcl

      - name: Validate docker-compose files
        run: |
          docker compose -f infrastructure/docker/docker-compose.yml config --quiet
          docker compose -f infrastructure/docker/docker-compose.yml \
                        -f infrastructure/docker/docker-compose.scan-mode.yml config --quiet
          docker compose -f infrastructure/docker/docker-compose.yml \
                        -f infrastructure/docker/docker-compose.online.yml config --quiet

  # ═══════════════════════════════════════════════════════════════════════════
  # BUILD JOB: Creates qcow2 image, compresses, splits, uploads to MinIO
  # ═══════════════════════════════════════════════════════════════════════════
  build:
    name: Build Appliance
    runs-on: self-hosted
    needs: validate
    timeout-minutes: 90
    outputs:
      version: ${{ steps.version.outputs.VERSION }}
      artifact-name: ${{ steps.version.outputs.ARTIFACT_NAME }}
    env:
      VERSION: ${{ inputs.version || github.ref_name }}
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@v2
        with:
          egress-policy: audit
          allowed-endpoints: >
            github.com:443
            api.github.com:443
            ghcr.io:443
            registry-1.docker.io:443
            auth.docker.io:443
            production.cloudflare.docker.com:443
            cdimage.debian.org:443
            deb.debian.org:443
            security.debian.org:443
            registry.ollama.ai:443
            download.docker.com:443
            dl.min.io:443

      - uses: actions/checkout@v4

      # ═══════════════════════════════════════════════════════════
      # CLEANUP-ONLY MODE (manual artifact cleanup)
      # ═══════════════════════════════════════════════════════════

      - name: Cleanup old artifacts (if requested)
        if: ${{ inputs.cleanup_only == true }}
        run: |
          echo "Cleaning up old build artifacts..."
          rm -rf infrastructure/packer/output/ || true
          sudo fstrim -av || true
          echo "Cleanup complete. Exiting without build."
          exit 0

      # ═══════════════════════════════════════════════════════════
      # EARLY VALIDATION (fail fast before expensive build)
      # ═══════════════════════════════════════════════════════════

      - name: Set version output
        id: version
        run: |
          VERSION_CLEAN="${VERSION#v}"
          echo "VERSION=$VERSION_CLEAN" >> $GITHUB_OUTPUT
          echo "ARTIFACT_NAME=appliance-${VERSION_CLEAN}" >> $GITHUB_OUTPUT
          echo "Building version: $VERSION_CLEAN"

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y unzip zstd qemu-utils

      - name: Setup Packer
        uses: hashicorp/setup-packer@v3
        with:
          version: ${{ env.PACKER_VERSION }}

      - name: Verify KVM support
        run: |
          if [[ ! -e /dev/kvm ]]; then
            echo "ERROR: KVM not available. Self-hosted runner requires hardware virtualization."
            exit 1
          fi
          ls -la /dev/kvm

      - name: Initialize Packer
        working-directory: infrastructure/packer
        run: packer init layer.pkr.hcl

      # ═══════════════════════════════════════════════════════════
      # BUILD PHASE (Layered: download base → build layer)
      # ═══════════════════════════════════════════════════════════

      - name: Download base image from MinIO
        env:
          MINIO_ENDPOINT: ${{ secrets.MINIO_ENDPOINT }}
          MINIO_ACCESS_KEY: ${{ secrets.MINIO_ACCESS_KEY }}
          MINIO_SECRET_KEY: ${{ secrets.MINIO_SECRET_KEY }}
          BASE_VERSION: ${{ inputs.base_version || 'latest' }}
        run: |
          # Install mc client
          wget -q https://dl.min.io/client/mc/release/linux-amd64/mc -O /tmp/mc
          chmod +x /tmp/mc

          # Configure alias
          /tmp/mc alias set minio $MINIO_ENDPOINT $MINIO_ACCESS_KEY $MINIO_SECRET_KEY

          # Create input directory
          mkdir -p infrastructure/packer/input

          # Download base image (no compression - saves only 0.3%)
          BASE_FILE="base-${BASE_VERSION}.qcow2"
          echo "Downloading base image: $BASE_FILE"
          /tmp/mc cp minio/appliance-base/${BASE_FILE} infrastructure/packer/input/

          # Rename to expected name
          cd infrastructure/packer/input
          mv ${BASE_FILE} base.qcow2
          ls -lh base.qcow2
          echo "BASE_IMAGE=input/base.qcow2" >> $GITHUB_ENV

      - name: Build layer qcow2 Image
        working-directory: infrastructure/packer
        run: |
          VERSION_CLEAN="${VERSION#v}"
          echo "Building layer for version: $VERSION_CLEAN"
          echo "Using base image: $BASE_IMAGE"
          packer build \
            -var "version=${VERSION_CLEAN}" \
            -var "base_image=${BASE_IMAGE}" \
            -on-error=abort \
            layer.pkr.hcl

      - name: Verify build output
        run: |
          ls -lh infrastructure/packer/output/
          QCOW2_FILE=$(ls infrastructure/packer/output/*.qcow2)
          echo "Built image: $QCOW2_FILE"
          echo "QCOW2_FILE=$QCOW2_FILE" >> $GITHUB_ENV

      - name: Compress with zstd
        run: |
          cd infrastructure/packer/output
          VERSION_CLEAN="${VERSION#v}"

          echo "Compressing with zstd (fast mode)..."
          zstd -1 -T0 -v *.qcow2 -o "network-agent-${VERSION_CLEAN}.qcow2.zst"

          echo "Compressed size:"
          ls -lh *.zst

          echo "Removing original qcow2 to free space..."
          rm -f *.qcow2
          df -h .

      - name: Split into parts
        run: |
          cd infrastructure/packer/output
          VERSION_CLEAN="${VERSION#v}"

          echo "Splitting into 1.9GB parts..."
          split -b 1900M -a 2 "network-agent-${VERSION_CLEAN}.qcow2.zst" "network-agent-${VERSION_CLEAN}.qcow2.zst.part-"

          # Rename numeric suffixes to alpha (aa, ab, ac, etc.)
          for f in *.part-[0-9][0-9]; do
            if [[ -f "$f" ]]; then
              num="${f##*.part-}"
              first=$(printf "\\x$(printf '%02x' $((97 + 10#$num / 26)))")
              second=$(printf "\\x$(printf '%02x' $((97 + 10#$num % 26)))")
              mv "$f" "${f%.part-*}.part-${first}${second}"
            fi
          done

          rm -f "network-agent-${VERSION_CLEAN}.qcow2.zst"
          echo "Parts created:"
          ls -lh *.part-*

      - name: Generate checksums
        run: |
          cd infrastructure/packer/output
          sha256sum *.part-* > SHA256SUMS
          cat SHA256SUMS

      - name: Copy install script
        run: |
          VERSION_CLEAN="${VERSION#v}"
          cp infrastructure/scripts/install-network-agent.sh infrastructure/packer/output/
          sed -i "s/VERSION:-0.4.0/VERSION:-${VERSION_CLEAN}/" infrastructure/packer/output/install-network-agent.sh

      # ═══════════════════════════════════════════════════════════
      # UPLOAD TO MINIO (fast LAN transfer for E2E test job)
      # ═══════════════════════════════════════════════════════════

      - name: Upload to MinIO
        env:
          MC_HOST_local: "${{ secrets.MINIO_ENDPOINT }}|${{ secrets.MINIO_ACCESS_KEY }}|${{ secrets.MINIO_SECRET_KEY }}"
        run: |
          # Install mc client
          wget -q https://dl.min.io/client/mc/release/linux-amd64/mc -O /tmp/mc
          chmod +x /tmp/mc

          # Configure alias (format: ALIAS/ACCESS_KEY/SECRET_KEY)
          /tmp/mc alias set minio ${{ secrets.MINIO_ENDPOINT }} ${{ secrets.MINIO_ACCESS_KEY }} ${{ secrets.MINIO_SECRET_KEY }}

          # Upload all artifacts
          VERSION_CLEAN="${VERSION#v}"
          /tmp/mc cp --recursive infrastructure/packer/output/ minio/appliance-builds/${VERSION_CLEAN}/

          # Verify upload
          echo "Uploaded to MinIO:"
          /tmp/mc ls minio/appliance-builds/${VERSION_CLEAN}/

      - name: Cleanup build directory
        if: always()
        run: |
          # Clean up after successful upload to free disk space
          rm -rf infrastructure/packer/output/ || true
          rm -rf infrastructure/packer/input/ || true
          sudo fstrim -av || true
          echo "Build artifacts uploaded to MinIO and cleaned up"

  # ═══════════════════════════════════════════════════════════════════════════
  # E2E TEST JOB: Downloads from MinIO, creates test VM, runs health checks
  # Can be re-run independently without rebuilding (saves ~40 min)
  # On release: Also uploads to GitHub Release after successful test
  # ═══════════════════════════════════════════════════════════════════════════
  e2e-test:
    name: E2E Test
    runs-on: self-hosted
    needs: build
    if: ${{ inputs.skip_test != true }}
    timeout-minutes: 30
    env:
      VERSION: ${{ needs.build.outputs.version }}
      ARTIFACT_NAME: ${{ needs.build.outputs.artifact-name }}
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@v2
        with:
          egress-policy: audit
          allowed-endpoints: >
            github.com:443
            api.github.com:443
            dl.min.io:443

      - uses: actions/checkout@v4

      - name: Validate Proxmox connectivity
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          # Check if PVE_HOST secret is configured
          if [[ -z "$PVE_HOST" ]]; then
            echo "::error::PVE_HOST secret is not configured!"
            echo ""
            echo "Please add the Proxmox host IP as a repository secret:"
            echo "  gh secret set PVE_HOST --body '10.0.0.69'"
            echo ""
            exit 1
          fi

          # Check SSH connectivity to Proxmox
          echo "Testing SSH connectivity to Proxmox ($PVE_HOST)..."
          if ! ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o BatchMode=yes root@$PVE_HOST "echo 'SSH OK'" 2>/dev/null; then
            echo "::error::Cannot SSH to Proxmox host $PVE_HOST"
            echo "Check: SSH key, firewall, Proxmox availability"
            exit 1
          fi
          echo "✓ Proxmox connectivity validated"

      - name: Download from MinIO
        env:
          MINIO_ENDPOINT: ${{ secrets.MINIO_ENDPOINT }}
          MINIO_ACCESS_KEY: ${{ secrets.MINIO_ACCESS_KEY }}
          MINIO_SECRET_KEY: ${{ secrets.MINIO_SECRET_KEY }}
        run: |
          # Install mc client
          wget -q https://dl.min.io/client/mc/release/linux-amd64/mc -O /tmp/mc
          chmod +x /tmp/mc

          # Configure alias
          /tmp/mc alias set minio $MINIO_ENDPOINT $MINIO_ACCESS_KEY $MINIO_SECRET_KEY

          # Download all artifacts
          mkdir -p infrastructure/packer/output
          /tmp/mc cp --recursive minio/appliance-builds/${VERSION}/ infrastructure/packer/output/

          echo "Downloaded artifacts:"
          ls -lh infrastructure/packer/output/
          echo ""
          echo "Verifying checksums..."
          cd infrastructure/packer/output
          sha256sum -c SHA256SUMS

      - name: Create test VM
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          # Clean up any existing test VM first
          ssh -o StrictHostKeyChecking=no root@$PVE_HOST "qm stop $TEST_VMID 2>/dev/null || true"
          ssh root@$PVE_HOST "qm destroy $TEST_VMID --purge 2>/dev/null || true"

          # Stream decompressed image directly to Proxmox (no temp files on runner)
          # Decompression on runner, only final qcow2 (32GB) goes over network
          BUILD_TMP="/root/build-tmp"
          ssh -o StrictHostKeyChecking=no root@$PVE_HOST "mkdir -p $BUILD_TMP"

          echo "Streaming decompressed image to Proxmox..."
          cd infrastructure/packer/output
          cat *.part-* | zstd -d | ssh root@$PVE_HOST "cat > $BUILD_TMP/network-agent-test.qcow2"

          echo "Verifying image on Proxmox..."
          ssh root@$PVE_HOST "ls -lh $BUILD_TMP/network-agent-test.qcow2"

          # Create VM on Proxmox host
          ssh root@$PVE_HOST "qm create $TEST_VMID \
            --name 'network-agent-test' \
            --memory 32768 \
            --cores 8 \
            --cpu host \
            --ostype l26 \
            --agent enabled=1 \
            --net0 virtio,bridge=vmbr0"

          # Import disk
          ssh root@$PVE_HOST "qm importdisk $TEST_VMID $BUILD_TMP/network-agent-test.qcow2 local-lvm --format qcow2"

          # Configure disk
          ssh root@$PVE_HOST "DISK=\$(pvesm list local-lvm | grep 'vm-${TEST_VMID}-disk' | awk '{print \$1}') && \
            qm set $TEST_VMID --scsi0 \"\$DISK\" --boot order=scsi0 --scsihw virtio-scsi-single"

          # Start VM
          ssh root@$PVE_HOST "qm start $TEST_VMID"

          # Cleanup temp file on Proxmox
          ssh root@$PVE_HOST "rm -f $BUILD_TMP/network-agent-test.qcow2"

      - name: Wait for boot and get IP
        id: vm-ip
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          echo "Waiting for VM to boot..."
          sleep 60

          VM_IP=""
          for i in {1..30}; do
            VM_IP=$(ssh root@$PVE_HOST "qm guest cmd $TEST_VMID network-get-interfaces" 2>/dev/null | \
                    jq -r '.[1]["ip-addresses"][]? | select(.["ip-address-type"] == "ipv4") | .["ip-address"]' 2>/dev/null | head -1 || echo "")

            if [[ -n "$VM_IP" && "$VM_IP" != "null" && "$VM_IP" != "127.0.0.1" ]]; then
              echo "VM_IP=$VM_IP" >> $GITHUB_OUTPUT
              echo "VM IP: $VM_IP"
              break
            fi
            echo "Waiting for IP... ($i/30)"
            sleep 10
          done

          if [[ -z "$VM_IP" || "$VM_IP" == "null" ]]; then
            echo "ERROR: Failed to get VM IP"
            echo ""
            echo "=== DEBUG: Network Interfaces ==="
            ssh root@$PVE_HOST "qm guest cmd $TEST_VMID network-get-interfaces" || true
            echo ""
            echo "=== DEBUG: systemd-networkd status ==="
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -lc 'systemctl is-enabled systemd-networkd; systemctl is-active systemd-networkd'" | jq -r '.["out-data"] // "no output"' || true
            echo ""
            echo "=== DEBUG: networkctl status ==="
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -lc 'networkctl status ens18 2>&1 || networkctl list'" | jq -r '.["out-data"] // "no output"' || true
            echo ""
            echo "=== DEBUG: ip link show ==="
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -lc 'ip link show'" | jq -r '.["out-data"] // "no output"' || true
            echo ""
            echo "=== DEBUG: journalctl systemd-networkd (last 50 lines) ==="
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -lc 'journalctl -b -u systemd-networkd --no-pager | tail -50'" | jq -r '.["out-data"] // "no output"' || true
            echo ""
            echo "=== DEBUG: /etc/systemd/network/ contents ==="
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -lc 'ls -la /etc/systemd/network/ && cat /etc/systemd/network/20-wired.network'" | jq -r '.["out-data"] // "no output"' || true
            echo ""
            echo "=== DEBUG: Other network managers ==="
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -lc 'systemctl is-active NetworkManager 2>/dev/null || echo inactive; systemctl is-active networking.service 2>/dev/null || echo inactive'" | jq -r '.["out-data"] // "no output"' || true
            exit 1
          fi

          # Inject runner's SSH key into VM (SSH hardening requires key auth)
          echo "Injecting SSH key for E2E testing..."
          RUNNER_PUBKEY=$(ssh-keygen -y -f ~/.ssh/id_ed25519 2>/dev/null || cat ~/.ssh/id_ed25519.pub 2>/dev/null || echo "")
          if [[ -n "$RUNNER_PUBKEY" ]]; then
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -c 'mkdir -p /root/.ssh && chmod 700 /root/.ssh && echo \"$RUNNER_PUBKEY\" >> /root/.ssh/authorized_keys && chmod 600 /root/.ssh/authorized_keys'"
            echo "SSH key injected successfully"
          else
            echo "WARNING: No SSH key found, SSH may fail"
          fi

          # Regenerate SSH host keys (deleted during build for unique identity)
          echo "Regenerating SSH host keys..."
          ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -c 'ssh-keygen -A && systemctl restart sshd'"
          echo "SSH host keys regenerated and sshd restarted"

          # Wait for SSH to be ready
          echo "Waiting for SSH to be ready..."
          for i in {1..10}; do
            if ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 -o BatchMode=yes root@$VM_IP "echo SSH ready" 2>/dev/null; then
              echo "SSH is ready"
              break
            fi
            echo "Waiting for SSH... ($i/10)"
            sleep 5
          done

      - name: Configure and start services
        run: |
          VM_IP="${{ steps.vm-ip.outputs.VM_IP }}"

          ssh -o StrictHostKeyChecking=no -o ConnectTimeout=30 root@$VM_IP bash << 'EOF'
            # Note: VERSION is NOT set - defaults to 'latest' which matches embedded images
            printf '%s\n' \
              'POSTGRES_USER=agent' \
              'POSTGRES_PASSWORD=test-password-123' \
              'BASIC_AUTH_USER=admin' \
              'BASIC_AUTH_HASH=$2a$14$test.hash.for.testing.only' \
              > /opt/network-agent/.env
            chmod 600 /opt/network-agent/.env

            mkdir -p /var/lib/network-agent
            touch /var/lib/network-agent/.initialized

            cd /opt/network-agent
            docker compose up -d
          EOF

      - name: Wait for services
        run: |
          VM_IP="${{ steps.vm-ip.outputs.VM_IP }}"
          echo "Waiting for services..."
          sleep 120
          ssh -o StrictHostKeyChecking=no root@$VM_IP "docker compose -f /opt/network-agent/docker-compose.yml ps"

      - name: Run health checks
        run: |
          VM_IP="${{ steps.vm-ip.outputs.VM_IP }}"

          echo "=== Checking Ollama ==="
          if ssh -o StrictHostKeyChecking=no root@$VM_IP "docker exec ollama curl -sf http://localhost:11434/api/tags | grep -q qwen3"; then
            echo "✓ Ollama model loaded"
          else
            echo "✗ Ollama model not found"
            ssh -o StrictHostKeyChecking=no root@$VM_IP "docker logs ollama --tail 50" || true
          fi

          echo "=== Checking PostgreSQL ==="
          if ssh -o StrictHostKeyChecking=no root@$VM_IP "docker exec postgres pg_isready -U agent"; then
            echo "✓ PostgreSQL is ready"
          else
            echo "✗ PostgreSQL not ready"
          fi

          echo "=== Final status ==="
          ssh -o StrictHostKeyChecking=no root@$VM_IP "docker compose -f /opt/network-agent/docker-compose.yml ps"

      - name: Cleanup test VM
        # Only cleanup on success - keep VM for debugging on failure
        if: success()
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          ssh root@$PVE_HOST "qm stop $TEST_VMID 2>/dev/null || true"
          sleep 5
          ssh root@$PVE_HOST "qm destroy $TEST_VMID --purge 2>/dev/null || true"

          # Clean up downloaded artifacts
          rm -rf infrastructure/packer/output/ || true

          # Reclaim disk space on runner (thin provisioned storage)
          echo "Running fstrim to reclaim disk space..."
          sudo fstrim -av || true

      - name: Cleanup test VM on failure
        if: failure()
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          # Cleanup VM and temp files on Proxmox
          if [[ -n "$PVE_HOST" ]]; then
            ssh -o StrictHostKeyChecking=no root@$PVE_HOST "qm stop $TEST_VMID 2>/dev/null || true"
            ssh root@$PVE_HOST "qm destroy $TEST_VMID --purge 2>/dev/null || true"
            ssh root@$PVE_HOST "rm -rf /root/build-tmp/network-agent*.qcow2* 2>/dev/null || true"
          fi

          # Clean up downloaded artifacts (MinIO cleanup handled by dedicated step)
          rm -rf infrastructure/packer/output/ || true
          echo "::notice::E2E failed. Re-run requires full rebuild (no artifact retention)."

          # Reclaim thin-provisioned space
          sudo fstrim -av || true
          echo "VM cleanup completed"

      # ═══════════════════════════════════════════════════════════
      # UPLOAD TO GITHUB RELEASE (only for releases, after successful E2E)
      # ═══════════════════════════════════════════════════════════

      - name: Upload to GitHub Release
        if: success() && github.event_name == 'release'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "Uploading to release ${{ github.event.release.tag_name }}..."
          cd infrastructure/packer/output
          gh release upload "${{ github.event.release.tag_name }}" \
            *.part-* \
            SHA256SUMS \
            install-network-agent.sh \
            --clobber
          echo "✓ Uploaded to GitHub Release"

      - name: Cleanup MinIO artifacts
        if: always()
        env:
          MINIO_ENDPOINT: ${{ secrets.MINIO_ENDPOINT }}
          MINIO_ACCESS_KEY: ${{ secrets.MINIO_ACCESS_KEY }}
          MINIO_SECRET_KEY: ${{ secrets.MINIO_SECRET_KEY }}
        run: |
          # Always clean up MinIO (no retention for daily builds)
          wget -q https://dl.min.io/client/mc/release/linux-amd64/mc -O /tmp/mc 2>/dev/null && chmod +x /tmp/mc
          /tmp/mc alias set minio $MINIO_ENDPOINT $MINIO_ACCESS_KEY $MINIO_SECRET_KEY 2>/dev/null || true
          /tmp/mc rm --recursive --force minio/appliance-builds/${VERSION}/ 2>/dev/null || true
          echo "✓ Cleaned up MinIO artifacts"
