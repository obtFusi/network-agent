name: Appliance Build

# ONE-CLICK APPLIANCE BUILD
# Builds a complete, ready-to-use VM appliance with everything included:
# - Debian 13 + Docker + Ollama + Models (from cache)
# - Network Agent Docker Images + Config
# - First-boot setup + Firewall
#
# Ollama models are cached on the runner (~15GB tarball) for fast builds.

on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Version (e.g., 0.10.1)'
        required: true
        type: string
      skip_test:
        description: 'Skip E2E test (for debugging)'
        type: boolean
        default: false
      cleanup_only:
        description: 'Only cleanup old artifacts (no build)'
        type: boolean
        default: false
  release:
    types: [published]

concurrency:
  group: appliance-build
  cancel-in-progress: false

env:
  PACKER_VERSION: "1.11.0"
  TEST_VMID: 999
  TELEMETRY_DIR: /tmp/telemetry
  TELEMETRY_BUCKET: appliance-telemetry

permissions:
  contents: write

jobs:
  validate:
    name: Validate Templates
    runs-on: ubuntu-latest
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@v2
        with:
          egress-policy: audit

      - uses: actions/checkout@v4

      - name: Setup Packer
        uses: hashicorp/setup-packer@v3
        with:
          version: ${{ env.PACKER_VERSION }}

      - name: Initialize Packer plugins
        working-directory: infrastructure/packer
        run: packer init base.pkr.hcl

      - name: Validate Packer template
        working-directory: infrastructure/packer
        run: |
          # Validate appliance template
          packer validate -var "version=0.0.0" base.pkr.hcl

      - name: Validate docker-compose files
        run: |
          docker compose -f infrastructure/docker/docker-compose.yml config --quiet
          docker compose -f infrastructure/docker/docker-compose.yml \
                        -f infrastructure/docker/docker-compose.scan-mode.yml config --quiet
          docker compose -f infrastructure/docker/docker-compose.yml \
                        -f infrastructure/docker/docker-compose.online.yml config --quiet

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # BUILD JOB: Complete appliance build from scratch (with Ollama cache)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  build:
    name: Build Appliance
    runs-on: self-hosted
    needs: validate
    timeout-minutes: 90
    outputs:
      version: ${{ steps.version.outputs.VERSION }}
      artifact-name: ${{ steps.version.outputs.ARTIFACT_NAME }}
    env:
      VERSION: ${{ inputs.version || github.ref_name }}
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@v2
        with:
          egress-policy: audit
          allowed-endpoints: >
            github.com:443
            api.github.com:443
            ghcr.io:443
            registry-1.docker.io:443
            auth.docker.io:443
            production.cloudflare.docker.com:443
            cdimage.debian.org:443
            deb.debian.org:443
            security.debian.org:443
            registry.ollama.ai:443
            download.docker.com:443
            dl.min.io:443

      - uses: actions/checkout@v4

      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # TELEMETRY INITIALIZATION
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      - name: Initialize telemetry
        id: telemetry
        run: |
          chmod +x infrastructure/scripts/telemetry.sh
          source infrastructure/scripts/telemetry.sh
          telemetry_init "build" "${VERSION#v}"
          # Make script available for all subsequent steps
          cp infrastructure/scripts/telemetry.sh /tmp/telemetry.sh

      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # CLEANUP-ONLY MODE (manual artifact cleanup)
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      - name: Cleanup old artifacts (if requested)
        if: ${{ inputs.cleanup_only == true }}
        run: |
          echo "Cleaning up old build artifacts..."
          rm -rf infrastructure/packer/output/ || true
          sudo fstrim -av || true
          echo "Cleanup complete. Exiting without build."
          exit 0

      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # EARLY VALIDATION (fail fast before expensive build)
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      - name: Set version output
        id: version
        run: |
          VERSION_CLEAN="${VERSION#v}"
          echo "VERSION=$VERSION_CLEAN" >> $GITHUB_OUTPUT
          echo "ARTIFACT_NAME=appliance-${VERSION_CLEAN}" >> $GITHUB_OUTPUT
          echo "Building version: $VERSION_CLEAN"

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y unzip zstd qemu-utils bc jq

      - name: Setup Packer
        uses: hashicorp/setup-packer@v3
        with:
          version: ${{ env.PACKER_VERSION }}

      - name: Verify KVM support
        run: |
          if [[ ! -e /dev/kvm ]]; then
            echo "ERROR: KVM not available. Self-hosted runner requires hardware virtualization."
            exit 1
          fi
          ls -la /dev/kvm

      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # OLLAMA CACHE (persistent on runner, saves 40GB download)
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      - name: Prepare Ollama model cache
        env:
          OLLAMA_MODEL: 'qwen3:30b-a3b'
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "ollama_cache_prepare"
          CACHE_DIR="/opt/ollama-cache"
          ARCHIVE_PATH="$CACHE_DIR/ollama-models.tar.zst"

          # Check if tarball cache exists (>1GB = valid)
          ARCHIVE_SIZE=$(stat -c%s "$ARCHIVE_PATH" 2>/dev/null || echo 0)
          if [[ "$ARCHIVE_SIZE" -gt 1000000000 ]]; then
            echo "âœ… Ollama cache exists ($(numfmt --to=iec $ARCHIVE_SIZE))"
            ls -lh "$ARCHIVE_PATH"
          else
            echo "ðŸ“¥ No cache found, downloading models..."
            sudo mkdir -p "$CACHE_DIR"
            sudo chown runner:runner "$CACHE_DIR"

            # Install Ollama on runner if not present
            if ! command -v ollama &> /dev/null; then
              curl -fsSL https://ollama.com/install.sh | sudo sh
            fi

            # Wait for Ollama service
            echo "â³ Waiting for Ollama service..."
            for i in {1..30}; do
              if curl -sf http://localhost:11434/api/tags > /dev/null 2>&1; then
                echo "âœ… Ollama ready"
                break
              fi
              sleep 2
            done

            # Ensure Ollama storage dir exists with correct permissions
            sudo mkdir -p /usr/share/ollama/.ollama
            sudo chown -R ollama:ollama /usr/share/ollama
            sudo systemctl restart ollama
            sleep 3

            # Pull models
            ollama pull "$OLLAMA_MODEL"
            ollama pull "qwen3:4b-instruct-2507-q4_K_M"

            # Create tarball DIRECTLY from Ollama storage (saves 40GB disk!)
            echo "ðŸ“¦ Creating tarball..."
            cd /usr/share/ollama/.ollama
            sudo tar -cf - models/ | zstd -3 -T0 > "$ARCHIVE_PATH"
            sudo chown runner:runner "$ARCHIVE_PATH"
            echo "âœ… Tarball: $(ls -lh "$ARCHIVE_PATH" | awk '{print $5}')"

            # DELETE models after tarball (saves 40GB)
            echo "ðŸ§¹ Cleaning up..."
            sudo rm -rf /usr/share/ollama/.ollama/models
            df -h /
          fi

          # Symlink for Packer HTTP server
          mkdir -p infrastructure/packer/http
          ln -sf "$ARCHIVE_PATH" infrastructure/packer/http/ollama-models.tar.zst

          telemetry_step_end "ollama_cache_prepare"

      - name: Initialize Packer
        working-directory: infrastructure/packer
        run: packer init base.pkr.hcl

      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # BUILD PHASE (complete appliance from Debian ISO)
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      - name: Build appliance qcow2 image
        working-directory: infrastructure/packer
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "packer_build"

          VERSION_CLEAN="${VERSION#v}"
          echo "Building complete appliance version: $VERSION_CLEAN"

          # Run packer and capture output for telemetry parsing
          packer build \
            -var "version=${VERSION_CLEAN}" \
            -on-error=abort \
            base.pkr.hcl 2>&1 | tee /tmp/packer-build.log

          telemetry_step_end "packer_build"

          # Extract Packer's internal telemetry from log
          echo "Extracting Packer step telemetry..."
          grep -E "â±ï¸|Duration:|CPU:|Memory:|Disk|Net " /tmp/packer-build.log > $TELEMETRY_DIR/packer_steps.txt || true

      - name: Verify build output
        run: |
          ls -lh infrastructure/packer/output/
          QCOW2_FILE=$(ls infrastructure/packer/output/*.qcow2)
          echo "Built image: $QCOW2_FILE"
          echo "QCOW2_FILE=$QCOW2_FILE" >> $GITHUB_ENV
          df -h .

      - name: Split into parts (no compression)
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "split_parts"

          cd infrastructure/packer/output
          VERSION_CLEAN="${VERSION#v}"

          # Rename qcow2 to final name
          QCOW2=$(ls *.qcow2)
          mv "$QCOW2" "network-agent-${VERSION_CLEAN}.qcow2"

          echo "Splitting into 1.9GB parts (no zstd - only 0.3% savings)..."
          split -b 1900M -a 2 "network-agent-${VERSION_CLEAN}.qcow2" "network-agent-${VERSION_CLEAN}.qcow2.part-"

          # Rename numeric suffixes to alpha (aa, ab, ac, etc.)
          for f in *.part-[0-9][0-9]; do
            if [[ -f "$f" ]]; then
              num="${f##*.part-}"
              first=$(printf "\\x$(printf '%02x' $((97 + 10#$num / 26)))")
              second=$(printf "\\x$(printf '%02x' $((97 + 10#$num % 26)))")
              mv "$f" "${f%.part-*}.part-${first}${second}"
            fi
          done

          # Delete original qcow2 after splitting
          rm -f "network-agent-${VERSION_CLEAN}.qcow2"
          echo "Parts created:"
          ls -lh *.part-*
          df -h .

          source /tmp/telemetry.sh
          telemetry_step_end "split_parts"

      - name: Generate checksums
        run: |
          cd infrastructure/packer/output
          sha256sum *.part-* > SHA256SUMS
          cat SHA256SUMS

      - name: Copy install script
        run: |
          VERSION_CLEAN="${VERSION#v}"
          cp infrastructure/scripts/install-network-agent.sh infrastructure/packer/output/
          sed -i "s/VERSION:-0.4.0/VERSION:-${VERSION_CLEAN}/" infrastructure/packer/output/install-network-agent.sh

      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # UPLOAD TO MINIO (fast LAN transfer for E2E test job)
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      - name: Upload to MinIO
        env:
          MC_HOST_local: "${{ secrets.MINIO_ENDPOINT }}|${{ secrets.MINIO_ACCESS_KEY }}|${{ secrets.MINIO_SECRET_KEY }}"
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "minio_upload"

          # Install mc client
          wget -q https://dl.min.io/client/mc/release/linux-amd64/mc -O /tmp/mc
          chmod +x /tmp/mc

          # Configure alias (format: ALIAS/ACCESS_KEY/SECRET_KEY)
          /tmp/mc alias set minio ${{ secrets.MINIO_ENDPOINT }} ${{ secrets.MINIO_ACCESS_KEY }} ${{ secrets.MINIO_SECRET_KEY }}

          # Upload all artifacts
          VERSION_CLEAN="${VERSION#v}"
          /tmp/mc cp --recursive infrastructure/packer/output/ minio/appliance-builds/${VERSION_CLEAN}/

          # Verify upload
          echo "Uploaded to MinIO:"
          /tmp/mc ls minio/appliance-builds/${VERSION_CLEAN}/

          telemetry_step_end "minio_upload"

      - name: Finalize and upload telemetry
        if: always()
        env:
          MINIO_ENDPOINT: ${{ secrets.MINIO_ENDPOINT }}
          MINIO_ACCESS_KEY: ${{ secrets.MINIO_ACCESS_KEY }}
          MINIO_SECRET_KEY: ${{ secrets.MINIO_SECRET_KEY }}
        run: |
          source /tmp/telemetry.sh
          telemetry_finalize

          # Upload telemetry to MinIO for persistent storage
          VERSION_CLEAN="${VERSION#v}"
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)

          wget -q https://dl.min.io/client/mc/release/linux-amd64/mc -O /tmp/mc 2>/dev/null || true
          chmod +x /tmp/mc 2>/dev/null || true

          /tmp/mc alias set minio $MINIO_ENDPOINT $MINIO_ACCESS_KEY $MINIO_SECRET_KEY 2>/dev/null || true

          # Create telemetry bucket if not exists
          /tmp/mc mb --ignore-existing minio/$TELEMETRY_BUCKET 2>/dev/null || true

          # Upload all telemetry files
          if [[ -d "$TELEMETRY_DIR" ]]; then
            /tmp/mc cp --recursive $TELEMETRY_DIR/ minio/$TELEMETRY_BUCKET/${VERSION_CLEAN}/${TIMESTAMP}/ 2>/dev/null || true
            echo "ðŸ“Š Telemetry uploaded to MinIO: $TELEMETRY_BUCKET/${VERSION_CLEAN}/${TIMESTAMP}/"
          fi

          # Also append to aggregate telemetry index
          TELEMETRY_JSON=$(ls $TELEMETRY_DIR/*.json 2>/dev/null | head -1)
          if [[ -f "$TELEMETRY_JSON" ]]; then
            /tmp/mc cp "$TELEMETRY_JSON" minio/$TELEMETRY_BUCKET/latest/build_${VERSION_CLEAN}.json 2>/dev/null || true
          fi

      - name: Cleanup build directory
        if: always()
        run: |
          # Clean up after successful upload to free disk space
          rm -rf infrastructure/packer/output/ || true
          rm -rf infrastructure/packer/input/ || true
          sudo fstrim -av || true
          echo "Build artifacts uploaded to MinIO and cleaned up"

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # E2E TEST JOB: Downloads from MinIO, creates test VM, runs health checks
  # Can be re-run independently without rebuilding (saves ~40 min)
  # On release: Also uploads to GitHub Release after successful test
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  e2e-test:
    name: E2E Test
    runs-on: self-hosted
    needs: build
    if: ${{ inputs.skip_test != true }}
    timeout-minutes: 30
    env:
      VERSION: ${{ needs.build.outputs.version }}
      ARTIFACT_NAME: ${{ needs.build.outputs.artifact-name }}
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@v2
        with:
          egress-policy: audit
          allowed-endpoints: >
            github.com:443
            api.github.com:443
            dl.min.io:443

      - uses: actions/checkout@v4

      - name: Initialize telemetry
        run: |
          chmod +x infrastructure/scripts/telemetry.sh
          source infrastructure/scripts/telemetry.sh
          telemetry_init "e2e_test" "${VERSION}"
          cp infrastructure/scripts/telemetry.sh /tmp/telemetry.sh

      - name: Validate Proxmox connectivity
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          # Check if PVE_HOST secret is configured
          if [[ -z "$PVE_HOST" ]]; then
            echo "::error::PVE_HOST secret is not configured!"
            echo ""
            echo "Please add the Proxmox host IP as a repository secret:"
            echo "  gh secret set PVE_HOST --body '10.0.0.69'"
            echo ""
            exit 1
          fi

          # Check SSH connectivity to Proxmox
          echo "Testing SSH connectivity to Proxmox ($PVE_HOST)..."
          if ! ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o BatchMode=yes root@$PVE_HOST "echo 'SSH OK'" 2>/dev/null; then
            echo "::error::Cannot SSH to Proxmox host $PVE_HOST"
            echo "Check: SSH key, firewall, Proxmox availability"
            exit 1
          fi
          echo "âœ“ Proxmox connectivity validated"

      - name: Download from MinIO
        env:
          MINIO_ENDPOINT: ${{ secrets.MINIO_ENDPOINT }}
          MINIO_ACCESS_KEY: ${{ secrets.MINIO_ACCESS_KEY }}
          MINIO_SECRET_KEY: ${{ secrets.MINIO_SECRET_KEY }}
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "minio_download"

          # Install mc client
          wget -q https://dl.min.io/client/mc/release/linux-amd64/mc -O /tmp/mc
          chmod +x /tmp/mc

          # Configure alias
          /tmp/mc alias set minio $MINIO_ENDPOINT $MINIO_ACCESS_KEY $MINIO_SECRET_KEY

          # Download all artifacts
          mkdir -p infrastructure/packer/output
          /tmp/mc cp --recursive minio/appliance-builds/${VERSION}/ infrastructure/packer/output/

          echo "Downloaded artifacts:"
          ls -lh infrastructure/packer/output/
          echo ""
          echo "Verifying checksums..."
          cd infrastructure/packer/output
          sha256sum -c SHA256SUMS

          source /tmp/telemetry.sh
          telemetry_step_end "minio_download"

      - name: Create test VM
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "vm_create"

          # Clean up any existing test VM first
          ssh -o StrictHostKeyChecking=no root@$PVE_HOST "qm stop $TEST_VMID 2>/dev/null || true"
          ssh root@$PVE_HOST "qm destroy $TEST_VMID --purge 2>/dev/null || true"

          # Stream image directly to Proxmox (no compression - only 0.3% savings)
          BUILD_TMP="/root/build-tmp"
          ssh -o StrictHostKeyChecking=no root@$PVE_HOST "mkdir -p $BUILD_TMP"

          echo "Streaming image parts to Proxmox..."
          cd infrastructure/packer/output
          cat *.part-* | ssh root@$PVE_HOST "cat > $BUILD_TMP/network-agent-test.qcow2"

          echo "Verifying image on Proxmox..."
          ssh root@$PVE_HOST "ls -lh $BUILD_TMP/network-agent-test.qcow2"

          # Create VM on Proxmox host
          ssh root@$PVE_HOST "qm create $TEST_VMID \
            --name 'network-agent-test' \
            --memory 32768 \
            --cores 8 \
            --cpu host \
            --ostype l26 \
            --agent enabled=1 \
            --net0 virtio,bridge=vmbr0"

          # Import disk
          ssh root@$PVE_HOST "qm importdisk $TEST_VMID $BUILD_TMP/network-agent-test.qcow2 local-lvm --format qcow2"

          # Configure disk
          ssh root@$PVE_HOST "DISK=\$(pvesm list local-lvm | grep 'vm-${TEST_VMID}-disk' | awk '{print \$1}') && \
            qm set $TEST_VMID --scsi0 \"\$DISK\" --boot order=scsi0 --scsihw virtio-scsi-single"

          # Start VM
          ssh root@$PVE_HOST "qm start $TEST_VMID"

          # Cleanup temp file on Proxmox
          ssh root@$PVE_HOST "rm -f $BUILD_TMP/network-agent-test.qcow2"

          source /tmp/telemetry.sh
          telemetry_step_end "vm_create"

      - name: Wait for boot and get IP
        id: vm-ip
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "vm_boot_wait"

          echo "Waiting for VM to boot..."
          sleep 60

          VM_IP=""
          for i in {1..30}; do
            VM_IP=$(ssh root@$PVE_HOST "qm guest cmd $TEST_VMID network-get-interfaces" 2>/dev/null | \
                    jq -r '.[1]["ip-addresses"][]? | select(.["ip-address-type"] == "ipv4") | .["ip-address"]' 2>/dev/null | head -1 || echo "")

            if [[ -n "$VM_IP" && "$VM_IP" != "null" && "$VM_IP" != "127.0.0.1" ]]; then
              echo "VM_IP=$VM_IP" >> $GITHUB_OUTPUT
              echo "VM IP: $VM_IP"
              break
            fi
            echo "Waiting for IP... ($i/30)"
            sleep 10
          done

          if [[ -z "$VM_IP" || "$VM_IP" == "null" ]]; then
            echo "ERROR: Failed to get VM IP"
            echo ""
            echo "=== DEBUG: Network Interfaces ==="
            ssh root@$PVE_HOST "qm guest cmd $TEST_VMID network-get-interfaces" || true
            echo ""
            echo "=== DEBUG: systemd-networkd status ==="
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -lc 'systemctl is-enabled systemd-networkd; systemctl is-active systemd-networkd'" | jq -r '.["out-data"] // "no output"' || true
            echo ""
            echo "=== DEBUG: networkctl status ==="
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -lc 'networkctl status ens18 2>&1 || networkctl list'" | jq -r '.["out-data"] // "no output"' || true
            echo ""
            echo "=== DEBUG: ip link show ==="
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -lc 'ip link show'" | jq -r '.["out-data"] // "no output"' || true
            echo ""
            echo "=== DEBUG: journalctl systemd-networkd (last 50 lines) ==="
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -lc 'journalctl -b -u systemd-networkd --no-pager | tail -50'" | jq -r '.["out-data"] // "no output"' || true
            echo ""
            echo "=== DEBUG: /etc/systemd/network/ contents ==="
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -lc 'ls -la /etc/systemd/network/ && cat /etc/systemd/network/20-wired.network'" | jq -r '.["out-data"] // "no output"' || true
            echo ""
            echo "=== DEBUG: Other network managers ==="
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -lc 'systemctl is-active NetworkManager 2>/dev/null || echo inactive; systemctl is-active networking.service 2>/dev/null || echo inactive'" | jq -r '.["out-data"] // "no output"' || true
            exit 1
          fi

          # Inject runner's SSH key into VM (SSH hardening requires key auth)
          echo "Injecting SSH key for E2E testing..."
          RUNNER_PUBKEY=$(ssh-keygen -y -f ~/.ssh/id_ed25519 2>/dev/null || cat ~/.ssh/id_ed25519.pub 2>/dev/null || echo "")
          if [[ -n "$RUNNER_PUBKEY" ]]; then
            ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -c 'mkdir -p /root/.ssh && chmod 700 /root/.ssh && echo \"$RUNNER_PUBKEY\" >> /root/.ssh/authorized_keys && chmod 600 /root/.ssh/authorized_keys'"
            echo "SSH key injected successfully"
          else
            echo "WARNING: No SSH key found, SSH may fail"
          fi

          # Regenerate SSH host keys (deleted during build for unique identity)
          echo "Regenerating SSH host keys..."
          ssh root@$PVE_HOST "qm guest exec $TEST_VMID -- bash -c 'ssh-keygen -A && systemctl restart sshd'"
          echo "SSH host keys regenerated and sshd restarted"

          # Wait for SSH to be ready
          echo "Waiting for SSH to be ready..."
          for i in {1..10}; do
            if ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 -o BatchMode=yes root@$VM_IP "echo SSH ready" 2>/dev/null; then
              echo "SSH is ready"
              break
            fi
            echo "Waiting for SSH... ($i/10)"
            sleep 5
          done

          source /tmp/telemetry.sh
          telemetry_step_end "vm_boot_wait"

      - name: Configure and start services
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "services_start"

          VM_IP="${{ steps.vm-ip.outputs.VM_IP }}"

          ssh -o StrictHostKeyChecking=no -o ConnectTimeout=30 root@$VM_IP bash << 'EOF'
            # Note: VERSION is NOT set - defaults to 'latest' which matches embedded images
            printf '%s\n' \
              'POSTGRES_USER=agent' \
              'POSTGRES_PASSWORD=test-password-123' \
              'BASIC_AUTH_USER=admin' \
              'BASIC_AUTH_HASH=$2a$14$test.hash.for.testing.only' \
              > /opt/network-agent/.env
            chmod 600 /opt/network-agent/.env

            mkdir -p /var/lib/network-agent
            touch /var/lib/network-agent/.initialized

            cd /opt/network-agent
            docker compose up -d
          EOF

          source /tmp/telemetry.sh
          telemetry_step_end "services_start"

      - name: Wait for services
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "services_wait"

          VM_IP="${{ steps.vm-ip.outputs.VM_IP }}"
          echo "Waiting for services..."
          sleep 120
          ssh -o StrictHostKeyChecking=no root@$VM_IP "docker compose -f /opt/network-agent/docker-compose.yml ps"

          telemetry_step_end "services_wait"

      - name: Run health checks
        run: |
          source /tmp/telemetry.sh
          telemetry_step_start "health_checks"

          VM_IP="${{ steps.vm-ip.outputs.VM_IP }}"

          echo "=== Checking Ollama ==="
          if ssh -o StrictHostKeyChecking=no root@$VM_IP "docker exec ollama curl -sf http://localhost:11434/api/tags | grep -q qwen3"; then
            echo "âœ“ Ollama model loaded"
          else
            echo "âœ— Ollama model not found"
            ssh -o StrictHostKeyChecking=no root@$VM_IP "docker logs ollama --tail 50" || true
          fi

          echo "=== Checking PostgreSQL ==="
          if ssh -o StrictHostKeyChecking=no root@$VM_IP "docker exec postgres pg_isready -U agent"; then
            echo "âœ“ PostgreSQL is ready"
          else
            echo "âœ— PostgreSQL not ready"
          fi

          echo "=== Final status ==="
          ssh -o StrictHostKeyChecking=no root@$VM_IP "docker compose -f /opt/network-agent/docker-compose.yml ps"

          source /tmp/telemetry.sh
          telemetry_step_end "health_checks"

      - name: Finalize and upload E2E telemetry
        if: always()
        env:
          MINIO_ENDPOINT: ${{ secrets.MINIO_ENDPOINT }}
          MINIO_ACCESS_KEY: ${{ secrets.MINIO_ACCESS_KEY }}
          MINIO_SECRET_KEY: ${{ secrets.MINIO_SECRET_KEY }}
        run: |
          source /tmp/telemetry.sh
          telemetry_finalize

          # Upload telemetry to MinIO
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)

          wget -q https://dl.min.io/client/mc/release/linux-amd64/mc -O /tmp/mc 2>/dev/null || true
          chmod +x /tmp/mc 2>/dev/null || true
          /tmp/mc alias set minio $MINIO_ENDPOINT $MINIO_ACCESS_KEY $MINIO_SECRET_KEY 2>/dev/null || true
          /tmp/mc mb --ignore-existing minio/$TELEMETRY_BUCKET 2>/dev/null || true

          if [[ -d "$TELEMETRY_DIR" ]]; then
            /tmp/mc cp --recursive $TELEMETRY_DIR/ minio/$TELEMETRY_BUCKET/${VERSION}/${TIMESTAMP}/ 2>/dev/null || true
            echo "ðŸ“Š E2E Telemetry uploaded to MinIO: $TELEMETRY_BUCKET/${VERSION}/${TIMESTAMP}/"
          fi

          # Update latest E2E telemetry
          TELEMETRY_JSON=$(ls $TELEMETRY_DIR/*.json 2>/dev/null | head -1)
          if [[ -f "$TELEMETRY_JSON" ]]; then
            /tmp/mc cp "$TELEMETRY_JSON" minio/$TELEMETRY_BUCKET/latest/e2e_${VERSION}.json 2>/dev/null || true
          fi

      - name: Cleanup test VM
        # Only cleanup on success - keep VM for debugging on failure
        if: success()
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          ssh root@$PVE_HOST "qm stop $TEST_VMID 2>/dev/null || true"
          sleep 5
          ssh root@$PVE_HOST "qm destroy $TEST_VMID --purge 2>/dev/null || true"

          # Clean up downloaded artifacts
          rm -rf infrastructure/packer/output/ || true

          # Reclaim disk space on runner (thin provisioned storage)
          echo "Running fstrim to reclaim disk space..."
          sudo fstrim -av || true

      - name: Cleanup test VM on failure
        if: failure()
        env:
          PVE_HOST: ${{ secrets.PVE_HOST }}
        run: |
          # Cleanup VM and temp files on Proxmox
          if [[ -n "$PVE_HOST" ]]; then
            ssh -o StrictHostKeyChecking=no root@$PVE_HOST "qm stop $TEST_VMID 2>/dev/null || true"
            ssh root@$PVE_HOST "qm destroy $TEST_VMID --purge 2>/dev/null || true"
            ssh root@$PVE_HOST "rm -rf /root/build-tmp/network-agent*.qcow2* 2>/dev/null || true"
          fi

          # Clean up downloaded artifacts (MinIO cleanup handled by dedicated step)
          rm -rf infrastructure/packer/output/ || true
          echo "::notice::E2E failed. Re-run requires full rebuild (no artifact retention)."

          # Reclaim thin-provisioned space
          sudo fstrim -av || true
          echo "VM cleanup completed"

      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # UPLOAD TO GITHUB RELEASE (only for releases, after successful E2E)
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      - name: Upload to GitHub Release
        if: success() && github.event_name == 'release'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "Uploading to release ${{ github.event.release.tag_name }}..."
          cd infrastructure/packer/output
          gh release upload "${{ github.event.release.tag_name }}" \
            *.part-* \
            SHA256SUMS \
            install-network-agent.sh \
            --clobber
          echo "âœ“ Uploaded to GitHub Release"

      - name: Cleanup MinIO artifacts
        if: always()
        env:
          MINIO_ENDPOINT: ${{ secrets.MINIO_ENDPOINT }}
          MINIO_ACCESS_KEY: ${{ secrets.MINIO_ACCESS_KEY }}
          MINIO_SECRET_KEY: ${{ secrets.MINIO_SECRET_KEY }}
        run: |
          # Always clean up MinIO (no retention for daily builds)
          wget -q https://dl.min.io/client/mc/release/linux-amd64/mc -O /tmp/mc 2>/dev/null && chmod +x /tmp/mc
          /tmp/mc alias set minio $MINIO_ENDPOINT $MINIO_ACCESS_KEY $MINIO_SECRET_KEY 2>/dev/null || true
          /tmp/mc rm --recursive --force minio/appliance-builds/${VERSION}/ 2>/dev/null || true
          echo "âœ“ Cleaned up MinIO artifacts"
