llm:
  venice:
    model: "llama-3.3-70b"
    temperature: 0.7
    max_tokens: 4096
    base_url: "https://api.venice.ai/api/v1"

  perplexity:  # Für später
    model: "llama-3.1-sonar-large-128k-online"
    base_url: "https://api.perplexity.ai"

agent:
  max_iterations: 10
  verbose: true

tools:
  network:
    allowed_networks:
      - "192.168.0.0/16"
      - "10.0.0.0/8"
      - "172.16.0.0/12"
    timeout: 60  # seconds

# Scan-Limits (Guardrails)
scan:
  max_hosts: 65536      # Maximum Hosts pro Scan (/16)
  allow_public: true    # Öffentliche IPs erlauben (false = nur private)
  tcp_ports: "22,80,443,8080,3389,5900"  # Ports für TCP-Connect Scan
