# LLM Provider Konfiguration
# Funktioniert mit jeder OpenAI-kompatiblen API
#
# WICHTIG: Alle Werte müssen an deinen Provider angepasst werden!

llm:
  provider:
    # Model-ID (REQUIRED - abhängig vom Provider)
    # Beispiele: gpt-4, gpt-3.5-turbo, llama-3.3-70b, mistral-large, etc.
    model: ""  # <-- HIER DEIN MODEL EINTRAGEN

    # API Endpoint (REQUIRED)
    # Venice.ai:  https://api.venice.ai/api/v1
    # OpenAI:     https://api.openai.com/v1
    # Ollama:     http://localhost:11434/v1
    # LM Studio:  http://localhost:1234/v1
    # Together:   https://api.together.xyz/v1
    # Groq:       https://api.groq.com/openai/v1
    base_url: ""  # <-- HIER DEINE API URL EINTRAGEN

    temperature: 0.7
    max_tokens: 4096

    # Context-Limit (Optional - wird automatisch von /models abgefragt)
    # Nur setzen falls API keine Info liefert
    # max_context_tokens: 8192

agent:
  max_iterations: 10
  verbose: true

tools:
  network:
    # Erlaubte Netzwerke für Scans (CIDR)
    allowed_networks:
      - "192.168.0.0/16"
      - "10.0.0.0/8"
      - "172.16.0.0/12"
    timeout: 60

scan:
  max_hosts: 65536
  allow_public: true
  tcp_ports: "22,80,443,8080,3389,5900"
