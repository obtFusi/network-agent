# LLM Provider Configuration
# Works with any OpenAI-compatible API
#
# IMPORTANT: All values must be adapted to your provider!

llm:
  provider:
    # Model-ID (REQUIRED - depends on provider)
    # Examples: gpt-4, gpt-3.5-turbo, llama-3.3-70b, mistral-large, etc.
    model: ""  # <-- ENTER YOUR MODEL HERE

    # API Endpoint (REQUIRED)
    # Venice.ai:  https://api.venice.ai/api/v1
    # OpenAI:     https://api.openai.com/v1
    # Ollama:     http://localhost:11434/v1
    # LM Studio:  http://localhost:1234/v1
    # Together:   https://api.together.xyz/v1
    # Groq:       https://api.groq.com/openai/v1
    base_url: ""  # <-- ENTER YOUR API URL HERE

    temperature: 0.7
    max_tokens: 4096

    # Context-Limit (Optional - automatically queried from /models)
    # Only set if API doesn't provide info
    # max_context_tokens: 8192

agent:
  max_iterations: 10
  verbose: true

# Scan configuration (v5.3: Config as SSOT)
scan:
  # v5.3: Split for Discovery (fast, many hosts) vs PortScan (slow, few hosts)
  max_hosts_discovery: 65536    # /16 - ping_sweep can scan large networks
  max_hosts_portscan: 256       # /24 - port_scan/service_detect are slow

  # Exclusion list (Networks/IPs) - targets matching these are blocked
  exclude_ips: []
  # Example:
  # exclude_ips:
  #   - "192.168.1.0/24"    # Exclude entire subnet
  #   - "10.0.0.5"          # Exclude single IP

  # Default timeout in seconds for all scan tools
  timeout: 120

  # Default TCP ports for port_scan (used as fallback if no ports specified)
  tcp_ports: "22,80,443,8080,3389,5900"
