# Network Agent Appliance Configuration
# Pre-configured for local Ollama with Qwen3

llm:
  provider:
    # Qwen3 4B Instruct - Fast model for CPU-only systems (default)
    # Uses instruct variant (no thinking mode) for faster responses
    # For better quality with GPU/more RAM, use: qwen3:30b-a3b
    model: "qwen3:4b-instruct-2507-q4_K_M"

    # Local Ollama endpoint (container name via Docker networking)
    base_url: "http://ollama:11434/v1"

    temperature: 0.7
    max_tokens: 4096

    # Context window - matches OLLAMA_CONTEXT_LENGTH=8192
    max_context_tokens: 8192

  # Ollama-specific CPU optimizations (passed via API options)
  ollama:
    num_ctx: 8192       # Match OLLAMA_CONTEXT_LENGTH
    num_batch: 128      # Optimized for 4-core CPU (default 512 is for GPU)
    num_gpu: 0          # Explicit CPU-only (skip GPU detection)

agent:
  max_iterations: 10
  verbose: true

# Scan configuration
scan:
  # Discovery can scan large networks
  max_hosts_discovery: 65536    # /16

  # Port scanning is slower, limit to /24
  max_hosts_portscan: 256

  # Exclusion list (add internal/sensitive networks here)
  exclude_ips: []
  # Example:
  # exclude_ips:
  #   - "192.168.1.0/24"
  #   - "10.0.0.5"

  # Timeout for scan operations
  timeout: 120

  # Default ports for quick scans
  tcp_ports: "22,80,443,8080,3389,5900"
