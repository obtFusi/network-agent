# Network Agent Appliance Configuration
# Pre-configured for local Ollama with Qwen3

llm:
  provider:
    # Qwen3 4B Instruct - Fast model for CPU-only systems (default)
    # Uses instruct variant (no thinking mode) for faster responses
    # For better quality with GPU/more RAM, use: qwen3:30b-a3b
    model: "qwen3:4b-instruct-2507-q4_K_M"

    # Local Ollama endpoint (container name via Docker networking)
    base_url: "http://ollama:11434/v1"

    temperature: 0.7
    max_tokens: 4096

    # Context window - smaller = faster on CPU (4k is good balance)
    # Increase to 8192 or 16384 if you have fast CPU/lots of RAM
    max_context_tokens: 4096

  # Ollama-specific CPU optimizations (optional, passed via API options)
  # Uncomment to tune for your hardware:
  # ollama:
  #   num_ctx: 4096      # Context window (match max_context_tokens)
  #   num_batch: 128     # Smaller batch = better for CPU
  #   num_thread: 6      # CPU cores to use (default: all)

agent:
  max_iterations: 10
  verbose: true

# Scan configuration
scan:
  # Discovery can scan large networks
  max_hosts_discovery: 65536    # /16

  # Port scanning is slower, limit to /24
  max_hosts_portscan: 256

  # Exclusion list (add internal/sensitive networks here)
  exclude_ips: []
  # Example:
  # exclude_ips:
  #   - "192.168.1.0/24"
  #   - "10.0.0.5"

  # Timeout for scan operations
  timeout: 120

  # Default ports for quick scans
  tcp_ports: "22,80,443,8080,3389,5900"
